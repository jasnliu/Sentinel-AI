How does the AI component work in your solution?
Sentinel AI is an edge-AI wildfire monitoring node. Each node is a refurbished /low-end Android phone placed in a weather-protected box with a camera facing outward. A solar panel keeps the phone charged, which allows the system to run continuously in remote locations.
On the software side, the phone captures images on a configurable interval (default to every 5 seconds). Each frame is resized and normalized, then passed into a lightweight convolutional neural network classifier in the architecture of MobileNetV3. I evaluated multiple architectures - including ResNet-18/50 and MobileNet variants - and ultimately selected MobileNetV3 for deployment because it provides the best speed-accuracy trade-off on low-cost CPUs. The deployed model is quantized and runs fully on-device, producing a probability score that indicates how likely the scene contains fire (and/or smoke-like cues depending on the dataset).
To reduce false alarms, the app requires repeated detections across consecutive frames and a confidence threshold before sending an alert. When the threshold is met, the device sends a small alert payload to a central endpoint (for example, time, device ID, and confidence score). This endpoint can route notifications to a ranger station or fire department and helps coordinate follow-up actions. The key idea is: fast, local detection first, then a lightweight network signal for human response.

